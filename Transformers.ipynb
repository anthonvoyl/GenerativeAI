{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Transformers\n",
    "\n",
    "Source: Sansievero-2024\n",
    "\n",
    "Many trace the most recent wave of advances in generative AI to the introduction of a class of models called transformers in 2017.\n",
    "Their most well-known applications are large language models (LLMs). In this notebook, we’ll explore the core ideas behind transformers and how they work, with a focus on one of the most common applications: language modeling.\n",
    "\n",
    "At its core, a language model (LM) is a probabilistic model that learns to predict the next word (or token) in a sequence based on the preceding or surrounding words. Doing so captures the language’s underlying structure and patterns, allowing the model to generate realistic and coherent text. For example, given the sentence “I began my day eating”, an LM might predict the next word as “breakfast” with a high probability.\n",
    "\n",
    "Transformers are designed to handle long-range dependencies and complex relationships between words efficiently and expressively. For example, imagine that you want to use an LM to summarize a news article, which might contain hundreds or even thousands of words. Traditional LMs, such as RNNs, struggle with long contexts, so the summary might skip critical details from the beginning of the article. Transformer-based LMs, however, show strong results in this task. Besides high-quality generations, transformers have other properties, such as efficient parallelization of training, scalability, and knowledge transfer, making them\n",
    "popular and well suited for multiple tasks. At the heart of this innovation lies a mechanism called self-attention, which allows the model to weigh the importance of each word in the context of the entire sequence.\n",
    "\n",
    "## A Language Model in Action\n",
    "In this section, we will load and interact with an existing small (pretrained) transformer model to get a high-level understanding of how they work. We’ll pick a small model you can run directly in your hardware, but consider that the same principles apply to the larger (over 100 times larger!) and more powerful models that have since been released.\n",
    "\n",
    "### Tokenizing Text\n",
    "Let’s begin our journey to generate text based on an initial input. For example, given the phrase \"it was a dark and stormy\", we want the model to generate words to continue it. Models can’t receive text directly as input; their input must be data represented as numbers. To feed text into a model, we must first find a way to turn sequences into numbers. This process is called tokenization, a crucial step in any NLP pipeline.\n",
    "An easy option would be to split the text into individual characters and assign each a unique numerical ID. This scheme could be helpful for languages such as Chinese, where each character carries much information. In languages like English, this creates a small token vocabulary, and there will be few unknown tokens (characters not found during training) when running inference. However, this method requires many tokens torepresent a string, which is bad for performance and erases some of the structure and meaning of the text—a downside for accuracy. Each character carries little information, making it hard for the model to learn the underlying structure of the text.\n",
    "\n",
    "Another approach could be to split the text into individual words. While this lets us capture more meaning per token, it has the downsides that we need to deal with more unknown words (e.g., typos or slang), we need to deal with different forms of the same word (e.g., “run”, “runs”, and “running”), and we might end up with a very large vocabulary, which could easily be over half a million words for languages such as English.\n",
    "Modern tokenization strategies strike a balance between these two extremes, splitting the text into subwords that capture both the structure and meaning of the text while still being able to handle unknown words and different forms of the same word (Figure 2-3). Characters that are usually found together (like most frequent words) can be assigned a single token that represents the whole word or group. Long or complicated words, or words with many inflections, may be split into multiple tokens, where each one usually represents a meaningful section of the word.\n",
    "There is no single best tokenizer; each LM comes with its own. The differences between tokenizers reside in the number of tokens supported and the tokenization strategy. For example, the GPT-2 tokenizer averages 1.3 tokens per word.\n",
    "Let’s find out how the Qwen tokenizer handles a sentence. We’ll first use the transformers library to load the tokenizer corresponding to Qwen. Then we’ll run the input text (also called prompt) through the tokenizer to encode the string into numbers representing the tokens. We’ll use the decode() method to convert each ID back into its corresponding token for demonstration purposes:"
   ],
   "id": "e8f4ae472afa8b90"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "86dfb1cf2934dad8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:52:36.234285Z",
     "start_time": "2025-12-14T17:52:35.534436Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "prompt = \"It was a dark and stormy\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")\n",
    "input_ids = tokenizer(prompt).input_ids\n",
    "input_ids"
   ],
   "id": "7ff06624ecc39f46",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1589, 436, 253, 3605, 284, 43471]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:52:36.330408Z",
     "start_time": "2025-12-14T17:52:36.325277Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for t in input_ids:\n",
    "    print(t, \"\\t:\", tokenizer.decode(t))"
   ],
   "id": "5470bd78db9add60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1589 \t: It\n",
      "436 \t:  was\n",
      "253 \t:  a\n",
      "3605 \t:  dark\n",
      "284 \t:  and\n",
      "43471 \t:  stormy\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:52:37.055927Z",
     "start_time": "2025-12-14T17:52:36.498639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM-135M\")"
   ],
   "id": "f82ac7e8a4e1d9b4",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "4ca1d919d7494420"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:52:37.283558Z",
     "start_time": "2025-12-14T17:52:37.092779Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# We tokenize again but specifying the tokenizer that we want it to\n",
    "# return a PyTorch tensor, which is what the model expects,# rather than a list of integers\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "outputs = model(input_ids)\n",
    "outputs.logits.shape # An output for each input token"
   ],
   "id": "b71aa112ccefb57a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 49152])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "- The first dimension of the output is the number of batches (1 because we just ran a single sequence through the model).\n",
    "- The second dimension is the sequence length, or the number of tokens in the input sequence (6 in this case).\n",
    "- The third dimension is the vocabulary size. We get a list of ~50,000 numbers for each token in the original sequence.\n",
    "These are the raw model outputs, or logits, that correspond to the tokens in the vocabulary. For every input token, the model predicts how likely each token in the vocabulary is to continue the sequence up to that point. With our example sentence, the model will predict logits for “It”, “It was”, “It was a”, and so on. Higher logit values mean the model considers the corresponding token a more likely continuation of the sequence. Table 2-1 shows the input sequences, the most likely token ID, and its corresponding token."
   ],
   "id": "95ea83b64e22fd34"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We get for each token a series of ~50000 logits (for each token in the tokenizer's vocabulary). Here we retrieve the series for token 2 int he prompt",
   "id": "95abc9c31f9fa7d0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:52:37.482557Z",
     "start_time": "2025-12-14T17:52:37.338623Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p2_logits = model(input_ids).logits[0, 0]\n",
    "p2_logits"
   ],
   "id": "d52e8de3709e39c4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([14.2227,  2.9400,  2.8643,  ...,  9.5122,  9.8202,  8.5702],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:52:37.518679Z",
     "start_time": "2025-12-14T17:52:37.511801Z"
    }
   },
   "cell_type": "code",
   "source": "p2_logits.argmax()",
   "id": "9ddf5c3cc6c09a51",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(314)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T17:52:37.718209Z",
     "start_time": "2025-12-14T17:52:37.713102Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer.decode(p2_logits.argmax())",
   "id": "2f1df9f559f00033",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b38f6b1d89a45dda"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:03:39.767454Z",
     "start_time": "2025-12-14T18:03:39.642969Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Last series of logits (i.e. after the last token in the prompt)\n",
    "p7_logits = model(input_ids).logits[0, -1]\n",
    "p7_logits.argmax()"
   ],
   "id": "240cd0c7d09ba80b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3163)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:03:53.193555Z",
     "start_time": "2025-12-14T18:03:53.185600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Token corresponding to id 3163\n",
    "tokenizer.decode(p7_logits.argmax())"
   ],
   "id": "b8bad6a95899df0b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' night'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T18:11:50.139558Z",
     "start_time": "2025-12-14T18:11:50.130089Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Other potential candidates\n",
    "import torch\n",
    "\n",
    "top10_logits = torch.topk(p7_logits, 10)\n",
    "for index in top10_logits.indices:\n",
    "    print(tokenizer.decode(index))"
   ],
   "id": "3f9aa0efb46471a5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " night\n",
      " day\n",
      " time\n",
      " evening\n",
      " winter\n",
      " sea\n",
      " morning\n",
      " month\n",
      " summer\n",
      " afternoon\n"
     ]
    }
   ],
   "execution_count": 36
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
